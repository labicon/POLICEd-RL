{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kartik\\anaconda3\\envs\\srlnbc\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from models.continuous_policy import Policy\n",
    "from models.critic import Value\n",
    "from models.discrete_policy import DiscretePolicy\n",
    "\n",
    "from algos.mujocoEnv import MujocoEnv\n",
    "import pickle, time\n",
    "# PATH = \"C:/Users/Kartik/My Drive (kartiknagpal@berkeley.edu)/ResearchCode/graph-transformer-tamp/baselines/PyTorch-CPO/assets/learned_models/CPO/Mujoco-Env/2024-02-02-exp-2-mujocoEnv/model.p\"\n",
    "\n",
    "env = MujocoEnv()\n",
    "\n",
    "# checkpoint = torch.load(\"./CPO-Best.pt\")\n",
    "# print(checkpoint)\n",
    "\n",
    "policy, value, running_state = pickle.load(open(\"./assets/learned_models/CPO/Mujoco-Env/2024-02-02-exp-5-mujocoEnv/model.p\", \"rb\"))\n",
    "# baselines\\PyTorch-CPO\\assets\\learned_models\\CPO\\Mujoco-Env\\2024-02-02-exp-2-mujocoEnv\\model.p\n",
    "# policy.to(device)\n",
    "# value.to(device)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while num_steps < min_batch_size:\n",
    "    state = env.reset()\n",
    "    if running_state is not None:\n",
    "        state = running_state(state)\n",
    "    reward_episode = 0\n",
    "    env_reward_episode = 0\n",
    "    reward_episode_list_1 = []\n",
    "    env_reward_episode_list_1 = []\n",
    "    episode_constraint_respect = True\n",
    "    episode_completed = False\n",
    "    \n",
    "    for t in range(1):\n",
    "        state_var = tensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            if mean_action:\n",
    "                action = policy(state_var)[0][0].numpy()\n",
    "            else:\n",
    "                action = policy.select_action(state_var)[0].numpy()\n",
    "        action = action.astype(np.float64)\n",
    "        next_state, reward, done, respect = env.step(action)\n",
    "        env.viewer.sync()\n",
    "        env_reward_episode += reward\n",
    "        env_reward_episode_list_1.append(reward)\n",
    "\n",
    "        if running_state is not None:\n",
    "            next_state = running_state(next_state)\n",
    "\n",
    "        if not respect:\n",
    "            episode_constraint_respect = False\n",
    "\n",
    "        if done:\n",
    "            episode_completed = True\n",
    "            break\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kartik\\My Drive (kartiknagpal@berkeley.edu)\\ResearchCode\\graph-transformer-tamp\\baselines\\PyTorch-CPO\\algos\\mujocoEnv.py:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_state = torch.tensor(new_state, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:  1\n",
      "Episode:  2\n",
      "Episode:  3\n",
      "Episode:  4\n",
      "Episode:  5\n",
      "Episode:  6\n",
      "Episode:  7\n",
      "Episode:  8\n",
      "Episode:  9\n",
      "Reward CI:(-258.0754684448242, -298.93770895273593, -217.21322793691246, 40.86224050791172)\n",
      "Respect CI:(0.6, 0.230591282220163, 0.969408717779837, 0.36940871777983697)\n",
      "Percent Completion:0.0\n",
      "Percent Completion w/o Violation:0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "num_episodes = 10\n",
    "eval_stats = np.zeros((num_episodes, 3))\n",
    "# policy = policy_net\n",
    "\n",
    "# NOT SURE\n",
    "mean_action = True\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    if running_state is not None:\n",
    "        state = running_state(state)\n",
    "    reward_episode = 0\n",
    "    env_reward_episode = 0\n",
    "    reward_episode_list_1 = []\n",
    "    env_reward_episode_list_1 = []\n",
    "    episode_constraint_respect = True\n",
    "    episode_completed = False\n",
    "    print(\"Episode: \", episode)\n",
    "\n",
    "    for t in range(500):\n",
    "        state_var = state.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            if mean_action:\n",
    "                action = policy(state_var)[0][0].numpy()\n",
    "            else:\n",
    "                action = policy.select_action(state_var)[0].numpy()\n",
    "        action = np.nan_to_num(action.astype(np.float64))\n",
    "        next_state, reward, done, respect = env.step(action)\n",
    "        \n",
    "        # print(env.getJoints())\n",
    "        env.viewer.sync()\n",
    "        time.sleep(0.1)\n",
    "        env_reward_episode += reward\n",
    "        env_reward_episode_list_1.append(reward)\n",
    "\n",
    "        if running_state is not None:\n",
    "            next_state = running_state(next_state)\n",
    "\n",
    "        if not respect:\n",
    "            episode_constraint_respect = False\n",
    "\n",
    "        if done:\n",
    "            episode_completed = True\n",
    "            break\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    eval_stats[episode] = [env_reward_episode.cpu(), episode_constraint_respect, episode_completed]\n",
    "\n",
    "def get_confidence_interval(data, confidence=0.95):\n",
    "    m, se = np.mean(data), scipy.stats.sem(data)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
    "    return m, m-h, m+h, h\n",
    "\n",
    "\n",
    "print(\"Reward CI:\" + str(get_confidence_interval(eval_stats[:,0])))\n",
    "print(\"Respect CI:\" + str(get_confidence_interval(eval_stats[:,1])))\n",
    "print(\"Percent Completion:\" + str(100*sum(eval_stats[:,2])/num_episodes))\n",
    "eval_stats[eval_stats[:,1] == 0.0, 2] = 0.0\n",
    "print(\"Percent Completion w/o Violation:\" + str(100*sum(eval_stats[:,2])/num_episodes))\n",
    "\n",
    "# print(f\"Reward CI: {get_confidence_interval(eval_stats[:,0])} /t Average respect: {get_confidence_interval(eval_stats[:,1])} \\t Percent Completion: {100*sum(eval_stats[:,2])/num_episodes} %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "srlnbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
