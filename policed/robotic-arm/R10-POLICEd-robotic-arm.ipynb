{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18fe214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import math, copy, time\n",
    "import torch\n",
    "from TD3 import TD3\n",
    "\n",
    "from utils import ReplayBuffer, episode_stats_queue\n",
    "import numpy as np\n",
    "import mujoco as mj\n",
    "import mujoco.viewer as vi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3561de3",
   "metadata": {},
   "source": [
    "Mujoco Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2da967c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tues Nov 14 1:20:12 2023\n",
    "\n",
    "@author: Kartik Nagpal, Jean-Baptiste Bouvier\n",
    "\n",
    "Simple environment for the mujoco system, inspired from the gym environment.\n",
    "\"\"\"\n",
    "\n",
    "class MujocoEnv():\n",
    "    \"\"\"Mujoco system environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, target = torch.tensor([0.5, 0.5, 0.5], device=device),\n",
    "                       precision = 0.05,\n",
    "                       max_magnitude = 0.1,\n",
    "                       constraint_min = torch.tensor([0.41, 0.34, 0.57], device=device),\n",
    "                       constraint_max = torch.tensor([0.59, 0.66, 0.57], device=device),\n",
    "                       debug = False):\n",
    "\n",
    "        ### Environment Parameters\n",
    "        self.state_size = 10 # state dimension\n",
    "        self.action_size = 7 # input dimension\n",
    "        self.max_actuation = torch.tensor([2.97, 2.09, 2.97, 2.09, 2.97, 2.09, 3.05], device=device)\n",
    "        self.action_min = -1*self.max_actuation\n",
    "        self.action_max = 1*self.max_actuation\n",
    "        self.last_action = torch.zeros(1, self.action_size)\n",
    "        self.action_magnitude = max_magnitude\n",
    "\n",
    "        ### Mujoco Setup\n",
    "        self.model = mj.MjModel.from_xml_path(\"./robotic-arm-model/test_scene.xml\")\n",
    "        self.data = mj.MjData(self.model)\n",
    "        self.viewer = vi.launch_passive(self.model, self.data)\n",
    "        cam = self.viewer.cam\n",
    "        cam.azimuth = 145.0 ; cam.elevation = -19.25 ; cam.distance = 2.0\n",
    "        cam.lookat = np.array([ 0.2228265373149372 , 0.25857197832292195 , 0.7914213541765485 ])\n",
    "        \n",
    "        ### Target\n",
    "        self.target = target\n",
    "        self.precision = precision\n",
    "\n",
    "        ### Forbidden to cross the line between constraint_min and constraint_max\n",
    "        self.constraint_min = constraint_min\n",
    "        self.constraint_max = constraint_max\n",
    "        self.width_affine_area = 2*torch.tensor([0.09, 0.16, 0.05], device=device)\n",
    "\n",
    "        self.constraint_line_x1 = torch.tensor([constraint_min[0], constraint_min[1]], device=device)\n",
    "        self.constraint_line_x3 = torch.tensor([constraint_min[0], constraint_max[0]], device=device)\n",
    "        self.constraint_line_x2 = torch.tensor([constraint_min[1], constraint_max[1]], device=device)\n",
    "        self.constraint_line_x4 = torch.tensor([constraint_max[0], constraint_max[1]], device=device)\n",
    "        self.constraint_max[2] += self.width_affine_area[2] # Switching the corner\n",
    "        self._allowed_constraint_area()\n",
    "        \n",
    "        self.site_index = -2\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "\n",
    "    def _allowed_constraint_area(self):\n",
    "        \"\"\"Calculates where it is allowed.\"\"\"\n",
    "        self.allowed_constraint_area = torch.cat((torch.cat(self.constraint_min, self.action_min),\n",
    "                                                  torch.cat(self.constraint_max, self.action_max),\n",
    "                                                  self.constraint_max + self.width_affine_area,\n",
    "                                                  self.constraint_min + torch.tensor([self.width_affine_area[0:2], 0.0])))\n",
    "\n",
    "    def getJoints(self):\n",
    "        return torch.tensor(self.data.qpos, dtype=torch.float, device=device)\n",
    "    \n",
    "    def reward(self, state):\n",
    "        r = -torch.sum(torch.abs(state - self.target))\n",
    "        return r\n",
    "\n",
    "    def fullState(self):\n",
    "        # Based on the current state definition\n",
    "        return copy.deepcopy(torch.cat((self.state, self.last_action)))\n",
    "\n",
    "\n",
    "    \n",
    "    def crossing(self, new_state):\n",
    "        if((new_state > self.constraint_min).all() and (new_state < self.constraint_max).all()):\n",
    "            return True\n",
    "        if(torch.sign(new_state[2]-self.constraint_min[2]) != torch.sign(self.state[2]-self.constraint_min[2])):\n",
    "            if(new_state[0] > self.constraint_min[0] and new_state[0] < self.constraint_max[0]):\n",
    "                if(new_state[1] > self.constraint_min[1] and new_state[1] < self.constraint_max[1]):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def respect_constraint(self, new_state):\n",
    "        \"\"\"Checks whether new_state respects the constraint.\"\"\"\n",
    "        return not self.crossing(new_state)\n",
    "    \n",
    "    \n",
    "    def arrived(self):\n",
    "        \"\"\"Episode is over when target is reached within given precision.\"\"\"\n",
    "        return torch.linalg.vector_norm(self.target - self.state) < self.precision\n",
    "    \n",
    "    def _propagation(self, u):\n",
    "        \"\"\"Common background function for 'step' and 'training_step'.\"\"\"\n",
    "        u = torch.clamp(u, min=-self.action_magnitude, max=self.action_magnitude)\n",
    "        \n",
    "        self.data.qpos += u.cpu().numpy()\n",
    "        self.last_action = torch.tensor(self.data.qpos, dtype=torch.float, device=device)\n",
    "        mj.mj_step(self.model, self.data)\n",
    "        new_state = torch.tensor(self.data.site_xpos[self.site_index], dtype=torch.float, device=device)\n",
    "        respect = self.respect_constraint(new_state)\n",
    "        too_far = (self.last_action < self.action_min).any().item() or (self.last_action > self.action_max).any().item() or (new_state[2] < 0)\n",
    "\n",
    "        if(self.debug):\n",
    "            self.viewer.sync()\n",
    "            print(\"Propogation function:\", u, new_state, respect, too_far)\n",
    "\n",
    "        return new_state, respect, too_far\n",
    "            \n",
    "    def step(self, u):\n",
    "        \"\"\"Returns: new_state, reward, done, respect\"\"\"\n",
    "        self.last_state = self.state\n",
    "        new_state, respect, too_far = self._propagation(u)\n",
    "        \n",
    "        ### Update to new state only if constraint is not violated and hasn't exited allowed area\n",
    "        if not too_far and respect:\n",
    "            self.state = new_state\n",
    "        else:\n",
    "            self._propagation(-u)\n",
    "            # print(\"Violation!\")\n",
    "        \n",
    "        done = self.arrived()\n",
    "        reward = self.reward(self.state) + done - 2*too_far - 6*(not respect)\n",
    "        reward -= 20*torch.relu(self.last_action.abs().max(dim=1).values).item()\n",
    "\n",
    "        if(self.debug):\n",
    "            self.viewer.sync()\n",
    "            print(\"Step function: \", u, self.state, respect, too_far, reward)\n",
    "\n",
    "        return self.fullState(), reward, done, respect\n",
    "\n",
    "    def training_step(self, u):\n",
    "        \"\"\"Takes a step in the environment. If the constraint is violated or if\n",
    "        the states exits the admissible area, the state is maintained in place\n",
    "        and a penalty is assigned. This prevents too short trajectories.\n",
    "        'done' is only assigned if the target is reached.\"\"\"\n",
    "        return self.step(u)\n",
    "        \n",
    "\n",
    "    def reset(self, max_dist_target = None): # base value should allow full state space sampling\n",
    "        \"\"\"Resets the state of the linear system to a random state in [x_min, x_max].\"\"\"\n",
    "        self.last_action = (self.action_max - self.action_min) * torch.rand(self.action_size, device=device) + self.action_min\n",
    "\n",
    "        self.data.qpos = self.last_action.cpu()\n",
    "        mj.mj_step(self.model, self.data)\n",
    "        self.viewer.sync()\n",
    "        self.state = torch.tensor(self.data.site_xpos[self.site_index], dtype=torch.float, device=device)\n",
    "        self.last_state = self.state\n",
    "                \n",
    "        if max_dist_target is not None and torch.norm(self.state - self.target) > max_dist_target:\n",
    "            self.reset(max_dist_target)\n",
    "            return self.fullState()\n",
    "\n",
    "        if(self.debug):\n",
    "            self.viewer.sync()\n",
    "            print(self.last_action, self.state, torch.norm(self.state - self.target))\n",
    "\n",
    "        return self.fullState()\n",
    "    \n",
    "    def random_action(self):\n",
    "        \"\"\"Returns a uniformly random action within action space.\"\"\"\n",
    "        a = (self.action_max - self.action_min) * torch.rand(self.action_size) + self.action_min\n",
    "        return torch.clamp(a, min=-self.action_magnitude, max=self.action_magnitude, device=device)\n",
    "\n",
    "    def close(self):\n",
    "        self.viewer.close()\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c029b0",
   "metadata": {},
   "source": [
    "Policy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1216ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'buffer_priority_method': '', 'POLICEd': False, 'N_step': 100, 'rand_episodes': 0, 'num_episodes': 2500, 'prb_alpha': 1, 'policy_freq': 3, 'hidden_size': 128, 'batch_size': 512, 'discount': 1.0, 'tau': 0.01, 'reward_threshold': 1000, 'target': tensor([[0.5000, 0.5000, 0.5000]], device='cuda:0')}\n",
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "#%% Hyperparameters\n",
    "POLICEd = False\n",
    "N_step = 100                     # Length of each episode\n",
    "rand_episodes = 0                # Number of episodes where random policy is used\n",
    "num_episodes = 2500              # Total number of training episodes\n",
    "\n",
    "\n",
    "policy_freq = 3                  # Frequency in episodes of delayed policy updates\n",
    "in_constraint_freq = 100       # Frequency in episodes at which the initial state is chosen in the allowed constraint area\n",
    "\n",
    "hidden_size = 128                # Number of units per hidden layer\n",
    "batch_size = 512                 # Batch size for both actor and critic\n",
    "discount = 1.00                  # Discount factor\n",
    "tau = 0.01                       # Target network update rate\n",
    "\n",
    "prb_alpha = 1                    # Prioritized Experience Replay alpha value\n",
    "reward_threshold = 1000          # Minimum average reward to stop training\n",
    "buffer_priority_method = \"\"\n",
    "\n",
    "### Cornered target\n",
    "target = torch.tensor([[0.5, 0.5, 0.5]], device=device)\n",
    "precision = 0.1\n",
    "max_action_magnitude = 0.05\n",
    "debug = False\n",
    "\n",
    "# Mujoco Environment that we defined\n",
    "env = MujocoEnv(target=target, precision=precision, max_magnitude=max_action_magnitude, debug=debug)\n",
    "\n",
    "# Noise values for TD3\n",
    "noise = torch.full((env.action_size,), env.action_magnitude, device=device)\n",
    "expl_noise = 0.025*noise        # Std of Gaussian exploration noise\n",
    "policy_noise = 0.025*noise      # Noise added to target policy during critic update\n",
    "noise_clip = 0.05*noise        # Range to clip target policy noise\n",
    "\n",
    "noise = torch.full((env.action_size,), 0.0, device=device)\n",
    "expl_noise = 0.025*noise        # Std of Gaussian exploration noise\n",
    "policy_noise = 0.025*noise      # Noise added to target policy during critic update\n",
    "noise_clip = 0.05*noise        # Range to clip target policy noise\n",
    "\n",
    "\n",
    "hyperparameters = {\n",
    "    \"buffer_priority_method\": buffer_priority_method,\n",
    "    \"POLICEd\": POLICEd,\n",
    "    \"N_step\": N_step,\n",
    "    \"rand_episodes\": rand_episodes,\n",
    "    \"num_episodes\": num_episodes,\n",
    "    \"prb_alpha\": prb_alpha,\n",
    "    \"policy_freq\": policy_freq,\n",
    "    \"hidden_size\": hidden_size,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"discount\": discount,\n",
    "    \"tau\": tau,\n",
    "    \"reward_threshold\": reward_threshold,\n",
    "    \"target\": target\n",
    "}\n",
    "\n",
    "print(hyperparameters)\n",
    "\n",
    "\n",
    "controller = TD3(env, POLICEd, hidden_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "replay_buffer = ReplayBuffer(env.state_size, env.action_size, max_size=1000*N_step, priority_method=buffer_priority_method, alpha=prb_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9618eb",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a417be",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_queue = episode_stats_queue(max_size=100)\n",
    "trained = False\n",
    "min_reward = -100000\n",
    "PATH = \"./checkpoints/rss_pos_3.pt\"\n",
    "all_stats = np.zeros((num_episodes, 3))\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # state = env.reset(episode/num_episodes + 0.3)\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_constraint_respect = True\n",
    "    episode_completion = False\n",
    "\n",
    "    for t in range(N_step):\n",
    "        ### Select action randomly or according to policy\n",
    "        if episode < rand_episodes:\n",
    "            action = env.random_action()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = controller.select_action(state)\n",
    "                action += torch.randn_like(action) * expl_noise\n",
    "\n",
    "        ### Perform action\n",
    "        next_state, reward, done, respect = env.training_step(action)\n",
    "        ### Store data in replay buffer\n",
    "        replay_buffer.add(state, action, next_state, reward, float(done))\n",
    "        state = next_state\n",
    "        \n",
    "        env.viewer.sync()\n",
    "        \n",
    "        episode_reward += reward\n",
    "        if not respect:\n",
    "            episode_constraint_respect = False\n",
    "\n",
    "        ### Train agent after collecting sufficient data\n",
    "        if episode >= rand_episodes:\n",
    "            controller.train(replay_buffer, batch_size)\n",
    "        if done: \n",
    "            episode_completion = True\n",
    "            break\n",
    "\n",
    "    all_stats[episode] = [episode_reward.cpu(), episode_constraint_respect, episode_completion]\n",
    "    stats_queue.add(episode_reward, float(episode_constraint_respect), float(episode_completion))\n",
    "    average_reward, average_respect = stats_queue.average()\n",
    "    ### Evaluate episodes\n",
    "    if episode % 50 == 0:\n",
    "        print(f\"Episode: {episode} \\t Average reward: {average_reward:.2f} \\t Average respect: {average_respect:.2f} \\t Percent Completion: {stats_queue.percentCompletion():.2f} %\")\n",
    "        # violation = empirical_constraint_violation(env, controller, 1000, True)\n",
    "        trained = average_reward > reward_threshold # and violation == 0.\n",
    "        # policy_map(env, controller, f\"Episode {episode}:\")\n",
    "        if(trained):\n",
    "            break\n",
    "    elif episode % 10 == 0:\n",
    "        # violation = empirical_constraint_violation(env, controller, 100, False)\n",
    "        trained = average_reward > reward_threshold # and violation == 0.\n",
    "        if(average_reward > min_reward):\n",
    "            torch.save({\n",
    "                'epoch': episode,\n",
    "                'actor_state_dict': controller.actor.net.state_dict(),\n",
    "                'actor_optimizer': controller.actor_optimizer.state_dict(),\n",
    "                'critic_Q1_state_dict': controller.critic.Q1_net.state_dict(),\n",
    "                'critic_Q2_state_dict': controller.critic.Q2_net.state_dict(),\n",
    "                'critic_optimizer': controller.critic_optimizer.state_dict(),\n",
    "                'average_reward': average_reward,\n",
    "                'average_respect': average_respect,\n",
    "                'all_rewards': all_stats,\n",
    "                'metadata': hyperparameters\n",
    "                }, PATH)\n",
    "            min_reward = average_reward\n",
    "        # elif(average_reward < min_reward-100): # Stop training if it's not working\n",
    "        #     trained = True\n",
    "        if(trained):\n",
    "            print(f\"FINISHED TRAINING - Episode: {episode} \\t Average reward: {average_reward:.2f} \\t Average respect: {average_respect:.2f} %\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d583c2a1",
   "metadata": {},
   "source": [
    "### Trajectory Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling out a trajectory\n",
    "state = env.reset()\n",
    "env.viewer.sync()\n",
    "episode_reward = 0\n",
    "\n",
    "N_step=100\n",
    "\n",
    "for t in range(N_step):\n",
    "    # step_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        action = controller.select_action(state)\n",
    "    print(state.cpu())#, action.cpu())\n",
    "    state, reward, done, respect = env.step(action)\n",
    "    # print(reward, env.reward(state[:3])) #reward,\n",
    "    reward = -torch.sum(torch.abs(state[:3] - env.target))\n",
    "    episode_reward += reward\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    env.viewer.sync()\n",
    "    \n",
    "    if done:\n",
    "       print(\"DONE\")\n",
    "       break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4352466a",
   "metadata": {},
   "source": [
    "#### Manual Save Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7572c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving and Loading checkpoints\n",
    "average_reward, average_respect = stats_queue.average()\n",
    "PATH = \"./model.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': num_episodes,\n",
    "    'actor_state_dict': controller.actor.net.state_dict(),\n",
    "    'actor_optimizer': controller.actor_optimizer.state_dict(),\n",
    "    'critic_Q1_state_dict': controller.critic.Q1_net.state_dict(),\n",
    "    'critic_Q2_state_dict': controller.critic.Q2_net.state_dict(),\n",
    "    'critic_optimizer': controller.critic_optimizer.state_dict(),\n",
    "    'average_reward': average_reward,\n",
    "    'average_respect': average_respect,\n",
    "    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb3ed8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf422f4",
   "metadata": {},
   "source": [
    "#### Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0832b3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n",
      "tensor(-67.1876) 0.8169014084507042\n"
     ]
    }
   ],
   "source": [
    "PATH = \"./checkpoints/unconstrained_full_6_uncliped.pt\"\n",
    "\n",
    "env = MujocoEnv(target=target, precision=precision, max_magnitude=max_action_magnitude, debug=debug)\n",
    "controller = TD3(env, POLICEd, hidden_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
    "replay_buffer = ReplayBuffer(env.state_size, env.action_size, max_size=1000*N_step, priority_method=buffer_priority_method, alpha=prb_alpha)\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "controller.actor.net.load_state_dict(checkpoint['actor_state_dict'])\n",
    "controller.actor_optimizer.load_state_dict(checkpoint['actor_optimizer'])\n",
    "controller.critic.Q1_net.load_state_dict(checkpoint['critic_Q1_state_dict'])\n",
    "controller.critic.Q2_net.load_state_dict(checkpoint['critic_Q2_state_dict'])\n",
    "controller.critic_optimizer.load_state_dict(checkpoint['critic_optimizer'])\n",
    "epoch = checkpoint['epoch']\n",
    "average_reward = checkpoint['average_reward']\n",
    "average_respect = checkpoint['average_respect']\n",
    "print(average_reward.cpu(), average_respect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abf2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = checkpoint['all_rewards']\n",
    "data = data[data[:,0] != 0., :]\n",
    "data = np.insert(data, 0, 100, axis=1)\n",
    "print(data)\n",
    "\n",
    "np.savetxt(\"./plotting/POLICEd_3.csv\", data, delimiter=\",\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cabba0",
   "metadata": {},
   "source": [
    "#### Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4dcdcb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward CI:(-25.02001190185547, -25.02001190185547, -25.02001190185547, 0.0)\n",
      "Respect CI:(1.0, 1.0, 1.0, 0.0)\n",
      "Percent Completion:0.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "num_episodes = 500\n",
    "eval_stats = np.zeros((num_episodes, 3))\n",
    "\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_constraint_respect = True\n",
    "    episode_completion = False\n",
    "\n",
    "    for t in range(N_step):\n",
    "        ### Select action randomly or according to policy\n",
    "        with torch.no_grad():\n",
    "            action = controller.select_action(state)\n",
    "            # action += torch.randn_like(action) * expl_noise\n",
    "\n",
    "        ### Perform action\n",
    "        next_state, reward, done, respect = env.training_step(action)\n",
    "        ### Store data in replay buffer\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if not respect:\n",
    "            episode_constraint_respect = False\n",
    "        if done:\n",
    "            episode_completion = True\n",
    "            break\n",
    "\n",
    "    eval_stats[episode] = [episode_reward.cpu(), episode_constraint_respect, episode_completion]\n",
    "\n",
    "def get_confidence_interval(data, confidence=0.95):\n",
    "    m, se = np.mean(data), scipy.stats.sem(data)\n",
    "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., len(data)-1)\n",
    "    return m, m-h, m+h, h\n",
    "\n",
    "\n",
    "print(\"Reward CI:\" + str(get_confidence_interval(eval_stats[:,0])))\n",
    "print(\"Respect CI:\" + str(get_confidence_interval(eval_stats[:,1])))\n",
    "print(\"Percent Completion:\" + str(100*sum(eval_stats[:,2])/num_episodes))\n",
    "\n",
    "#print(f\"Reward CI: {get_confidence_interval(eval_stats[:,0])} \\t Average respect: {get_confidence_interval(eval_stats[:,1])} \\t Percent Completion: {100*sum(eval_stats[:,2])/num_episodes} %\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
